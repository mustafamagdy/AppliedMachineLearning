{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNjbleaNrsOR"
      },
      "source": [
        "# Numeric data for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndyo4BqnrsOT"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Before we can do anything with machine learning, we need to load our data—and one of the most common formats for storing data is CSV, which stands for *Comma-Separated Values*. A CSV file is simply a plain text file where each row represents a record (like a person, a review, or a product), and each value is separated by a comma.\n",
        "\n",
        "Python gives us a few ways to load CSV files into a program. The most common methods are:\n",
        "\n",
        "1. Using Python’s built-in `csv` module  \n",
        "2. Using NumPy’s functions like `loadtxt` or `genfromtxt` (useful for numbers)  \n",
        "3. Using the `pandas` library and its `read_csv()` function (the most flexible and beginner-friendly)\n",
        "\n",
        "Each method has its own strengths:\n",
        "\n",
        "- *Python’s csv module* is built in—you don’t need to install anything—but it requires a bit more effort to convert data into the types you need (like numbers or dates).\n",
        "- *NumPy* is great for large numerical datasets, but it can struggle with files that contain text, headers, or missing values.\n",
        "- *pandas* is the most popular option because it makes working with tables of data very easy. It automatically handles many of the common problems you might run into and gives you a powerful tool called a *DataFrame*—a table that you can search, filter, and analyse with just a few lines of code.\n",
        "\n",
        "If you’re curious about how CSV files are structured or why they work the way they do, there’s an official guide called [RFC 4180](https://tools.ietf.org/html/rfc4180) that describes the standard format.\n",
        "\n",
        "Here are a few things to check when loading a CSV file:\n",
        "\n",
        "- *Header row*: Does the first line of your file contain column names (like \"Name\", \"Age\", or \"Review\")? If so, most tools can detect and use it automatically. If not, you’ll need to tell the tool what the column names should be.\n",
        "\n",
        "- *Comments*: Some CSV files include comment lines that start with a symbol like `#`. These lines are meant for humans, not the program, so you may need to tell your loading function to skip them.\n",
        "\n",
        "- *Delimiter*: Although CSV stands for \"comma-separated\", sometimes other characters are used instead—like tabs (`\\t`) or semicolons (`;`). Make sure you specify the correct one so the data loads properly.\n",
        "\n",
        "- *Quotes*: Some values in a file might contain commas, spaces, or special characters. To avoid confusion, these fields are wrapped in quotation marks (usually `\"`). If your file uses a different type of quote (like `'`), you’ll need to set that too.\n",
        "\n",
        "If you understand how your CSV file is set up, you’ll be able to load it correctly and avoid common errors—getting you one step closer to analysing and learning from your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4btreo35rsOU"
      },
      "source": [
        "### Installing Python libraries\n",
        "One great feature about jupyter notebooks is that we can run terminal commands. This means we can install python libraries on the fly, using the `!` prefix. If you plan on running these notebooks on your own machine, you'll need to install a few libraries as and when they are required. Below is an example specifically installing `pandas` and `numpy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU2vdiBkbKBE",
        "outputId": "2a5d988d-0697-4758-c8b5-49101fbb6521"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install pandas numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNhuhRjlrsOU"
      },
      "source": [
        "## Pima Indians dataset\n",
        "\n",
        "We will use the **Pima Indians dataset** to demonstrate how to load data into python. This dataset describes medical records for Pima Indians, indicating whether or not each patient develops diabetes within five years.\n",
        "\n",
        "The Pima Indian diabetes dataset is a renowned benchmark in the field of machine learning. It was originally made available through the *National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK)* in the United States, and later hosted on the *UCI Machine Learning Repository*. Over time, it has become a standard reference dataset for illustrating and evaluating classification algorithms.\n",
        "\n",
        "### Who was studied\n",
        "\n",
        "- The dataset focuses on adult female patients (aged 21 or older) of Pima Indian heritage residing near Phoenix, Arizona.  \n",
        "- This population is known to have a significantly higher incidence of type 2 diabetes.\n",
        "### What was measured\n",
        "\n",
        "Each row in the dataset represents one person and includes *eight numbers* that describe their medical background and personal health measurements. These features help researchers and doctors predict the chances of developing type 2 diabetes.\n",
        "\n",
        "Here’s what each number means:\n",
        "\n",
        "1. *Number of times pregnant* – How many times the participant has been pregnant (applies only to female patients in the dataset).  \n",
        "2. *Plasma glucose concentration* – The amount of glucose (sugar) in the blood after a two-hour test where the person drinks a sugary solution.  \n",
        "3. *Diastolic blood pressure* – The lower of the two blood pressure numbers, measured in millimetres of mercury (mm Hg).  \n",
        "4. *Triceps skin fold thickness* – A measurement in millimetres of the fat under the skin at the back of the upper arm.  \n",
        "5. *2-hour serum insulin* – The level of insulin in the blood two hours after the glucose test, measured in micro-units per millilitre.  \n",
        "6. *Body Mass Index (BMI)* – A number that shows whether someone is underweight, normal, overweight, or obese, based on their weight and height.  \n",
        "7. *Diabetes Pedigree Function (DPF)* – A calculated value that estimates a person’s genetic risk of diabetes based on family history.  \n",
        "8. *Age* – The person’s age in years.\n",
        "\n",
        "Finally, there’s a *binary outcome* called the *class*, *target*, or *label*, which tells us whether that person *did* or *did not* develop type 2 diabetes within the next five years. This is what models will try to predict using the other measurements.\n",
        "\n",
        "### Why it was collected\n",
        "\n",
        "Researchers aimed to investigate risk factors for diabetes among a group with a particularly high risk of the disease. Various medical and demographic data (such as glucose tolerance tests, insulin measurements, and age) were gathered to determine which factors most strongly predicted the onset of diabetes.\n",
        "\n",
        "### How the data was obtained\n",
        "\n",
        "1. **Patient recruitment**  \n",
        "   Eligible participants were female Pima Indians, aged 21 or older.  \n",
        "\n",
        "2. **Measurements and testing**  \n",
        "   Each participant underwent standard medical tests, including measuring plasma glucose concentration, blood pressure, and insulin levels, alongside providing demographic details such as age and number of pregnancies.  \n",
        "\n",
        "3. **Five-year follow-up**  \n",
        "   The pivotal outcome was whether each participant experienced the onset of type 2 diabetes within five years. This information was determined through follow-up medical records and diagnoses.  \n",
        "\n",
        "4. **Compilation**  \n",
        "   The anonymised data were assembled into a structured dataset of 768 entries, each representing a single participant’s measurements and diabetes outcome (onset or no onset).\n",
        "\n",
        "The Pima Indian diabetes dataset remains a pivotal resource for demonstrating fundamental classification techniques and exploring how demographic and medical attributes can help predict the onset of type 2 diabetes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaQzlJLeWoHL"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qwv_0ynWsXC",
        "outputId": "6c871314-c6c3-4108-b1b2-5eafcc5c03ab"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/martyn-harris-bbk/AppliedMachineLearning/main/data/pima-indians-diabetes.data.csv'\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "print(\"Download complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkvWy8amrsOW"
      },
      "source": [
        "### Load csv from file\n",
        "\n",
        "We demonstrate how to read a CSV file from your local system. We do this by providing the file path in the `filename` variable, and specifying a list of column names (`header`). If your CSV already contains a header row, you could set `header=0` (or omit the `names=` parameter entirely) to tell pandas to use the first row of the file as the header.\n",
        "\n",
        "The `.read_csv()` function returns a `pandas.DataFrame` object, which is a powerful 2D data structure that allows row and column operations, descriptive statistics, and data manipulations. You can immediately start summarising and visualising the data using DataFrame methods such as `.describe()`, `.head()`, `.plot()`, and so on.\n",
        "\n",
        "For more information on `pandas.DataFrame`, see the [API documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEJxb5ADrsOW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "\n",
        "header = [\n",
        "    'Pregnancy_Count',\n",
        "    'Glucone_conc',\n",
        "    'Blood_pressure',\n",
        "    'Skin_thickness',\n",
        "    'Insulin',\n",
        "    'BMI',\n",
        "    'DPF',\n",
        "    'Age',\n",
        "    'Class'\n",
        "]\n",
        "\n",
        "data = pd.read_csv(filename, names=header)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWLpVqwIrsOW"
      },
      "source": [
        "### Load csv using pandas from url\n",
        "\n",
        "In many cases, you may want to load CSV data directly from a web resource. Below, we show you how to modify the example to read the Pima Indians data from a GitHub URL, without having to download it locally first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8uOWfBZrsOX"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/martyn-harris-bbk/AppliedMachineLearning/main/data/pima-indians-diabetes.data.csv'\n",
        "\n",
        "header = [\n",
        "    'Pregnancy_Count',\n",
        "    'Glucone_conc',\n",
        "    'Blood_pressure',\n",
        "    'Skin_thickness',\n",
        "    'Insulin',\n",
        "    'BMI',\n",
        "    'DPF',\n",
        "    'Age',\n",
        "    'Class'\n",
        "]\n",
        "\n",
        "data = pd.read_csv(url, names=header)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_Ow_DsyDrsOX",
        "outputId": "ae58b478-e297-4869-94e9-bea73902c812"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In7fBVPlrsOX"
      },
      "source": [
        "By default, `head()` returns 5 rows, but you can specify exactly how many rows you want to preview by passing an integer. For example, `data.head(20)` will show the first 20 rows.\n",
        "\n",
        "Previewing just the first few rows is extremely helpful for verifying that the data has been read in correctly, especially if you suspect issues with delimiters, headers, or quoting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "8x2Cr0mbrsOX",
        "outputId": "57e0cba5-4e9a-45c9-f4a0-a5a039e64787"
      },
      "outputs": [],
      "source": [
        "data.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVQXQ7A4rsOX"
      },
      "source": [
        "Similarly, `data.tail()` returns the last few rows of the DataFrame. This can help you inspect how data is structured near the end of the file and confirm if the file terminates properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-25pdt1OrsOX",
        "outputId": "1394ed55-87f0-4ef0-f9be-2d130d370758"
      },
      "outputs": [],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zw6XMGtrsOX"
      },
      "source": [
        "### What is the dimensionality of our data?\n",
        "\n",
        "Before we start analysing or training a model, it’s helpful to know the *shape* of the dataset—that is, how many rows and columns it contains. This gives us a sense of how big the dataset is and what we’re working with.\n",
        "\n",
        "We can check this using the `.shape` attribute in a pandas DataFrame, which returns a result like this:\n",
        "\n",
        "```\n",
        "(rows, columns)\n",
        "```\n",
        "\n",
        "- The number of *rows* tells us how many *examples* or *participants* are in the dataset.  \n",
        "- The number of *columns* tells us how many *features* (measurements) we have for each example. If the dataset includes the target variable (like whether someone developed diabetes), it will be one of these columns.\n",
        "\n",
        "Understanding the dataset’s shape helps us:\n",
        "- Plan how to split the data into training and testing sets  \n",
        "- Estimate how much memory we’ll need  \n",
        "- Choose models and techniques that are suitable for the size of the data  \n",
        "\n",
        "For example, a dataset with 768 rows and 9 columns means we have 768 people in the study, and for each person, we’ve recorded 8 health measurements plus 1 outcome label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J6PWDSMrsOY",
        "outputId": "19122728-758a-4adb-e687-489e5c6689e5"
      },
      "outputs": [],
      "source": [
        "# How big is our data\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67LvZQvorsOY"
      },
      "source": [
        "In more formal terms, from the tuple returned, we can see that the Pima Indians dataset contains **768 rows** and **9 columns**. The 9 columns correspond to 8 explanatory features plus 1 target column (`Class`).\n",
        "\n",
        "This knowledge helps us ensure we have the complete dataset loaded. It's also a good initial check before we proceed to more in-depth data profiling or feature analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P37fViu6rsOY"
      },
      "source": [
        "### Exploring more details of the data\n",
        "\n",
        "Beyond checking the first few and last few rows, pandas offers convenient methods to quickly summarise your dataset. For instance, if you want to view column data types and any non-null counts, you can use `data.info()`. This can be useful to spot missing values or confirm that columns are numeric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVuDY1PVrsOY",
        "outputId": "52e70c52-06f9-468f-9863-df81eaf9f962"
      },
      "outputs": [],
      "source": [
        "# Checking data info\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1HvwOnirsOY"
      },
      "source": [
        "From `data.info()`, we get a quick overview of the dataset’s structure. It tells us:\n",
        "\n",
        "- *How many rows are in the dataset*  \n",
        "- *Which columns are present*  \n",
        "- *How many non-null (non-missing) values are in each column*  \n",
        "- *What data type* each column uses (such as `int64` for whole numbers, `float64` for decimal numbers, or `object` for text)\n",
        "\n",
        "This information is especially helpful for spotting issues early. For example:\n",
        "- If a column has fewer non-null values than the total number of rows, it likely contains *missing data*.\n",
        "- If a column that should contain numbers is listed as `object`, the values might be stored as text—possibly due to formatting issues (like commas or symbols).\n",
        "\n",
        "Checking the results of `data.info()`, allows us to make sure the dataset is properly formatted and ready for analysis or model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preparation and transformation\n",
        "\n",
        "In the real world, data often doesn’t come in a neat and tidy format that’s ready for analysis or machine learning. Before we can build reliable models, we usually need to *prepare* and *transform* the data to make it suitable for the task at hand.\n",
        "\n",
        "This step is crucial—poorly prepared data can lead to misleading results, even if you’re using the most advanced algorithms. Good preparation helps models learn more effectively, make better predictions, and generalise well to new data.\n",
        "\n",
        "Here are some of the most common transformations applied to numeric data:\n",
        "\n",
        "- *Rescaling* – Changing the range of values (for example, so everything falls between 0 and 1). This is useful when your data includes features measured on very different scales.\n",
        "- *Standardising* – Shifting data so it has a mean of 0 and a standard deviation of 1. This helps many algorithms that assume the data is centred and scaled, such as linear models or clustering methods.\n",
        "- *Normalising* – Adjusting values so that each row (or sample) has the same overall scale, often useful when comparing data with different units or magnitudes.\n",
        "- *Binarising* – Turning numerical data into 0s and 1s, based on a threshold. This is useful when you want to highlight the presence or absence of a certain condition (e.g. blood pressure above a certain level).\n",
        "\n",
        "As an example, we use the *Pima Indians Diabetes dataset*—a real-world medical dataset where preparation is especially important. This dataset contains health-related measurements such as blood pressure, glucose levels, and BMI. These features are all measured on different scales, which makes it harder for some models to interpret them fairly without some kind of adjustment.\n",
        "\n",
        "When we apply these transformations, we ensure that each feature contributes appropriately to the model—rather than allowing one with larger numbers to dominate simply because of its scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rescale data\n",
        "\n",
        "Sometimes in a dataset, the different features or attributes are measured on very different scales. For example, one column might show *income* in the tens of thousands, while another shows *age* in single or double digits. If we leave them as they are, some machine learning algorithms may give more importance to the larger numbers—even if they’re not actually more important.\n",
        "\n",
        "*Rescaling* helps solve this problem by putting all features on a similar scale, usually between 0 and 1. This means that the smallest value in a column becomes 0, the largest becomes 1, and everything else is adjusted to fit in between.\n",
        "\n",
        "This technique is useful for:\n",
        "- *Ensuring that all features contribute fairly to a model*  \n",
        "- *Improving the performance and stability of algorithms like neural networks, k-nearest neighbours, and gradient descent*  \n",
        "- *Speeding up training time by making the data easier for the algorithm to process*\n",
        "\n",
        "Think of it like putting everything on the same ruler—when all the numbers are in the same range, comparisons between them become more balanced.\n",
        "\n",
        "For example, in a health dataset with *cholesterol level*, *heart rate*, and *age*, rescaling makes sure that no one measurement overshadows the others simply because it's measured on a larger scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler  # Import MinMaxScaler for feature scaling\n",
        "\n",
        "# Extract values from the DataFrame\n",
        "array = data.values\n",
        "\n",
        "# Separate features (columns 0 to 7) and target (column 8)\n",
        "X = array[:, 0:8]  # Features\n",
        "Y = array[:, 8]    # Target\n",
        "\n",
        "# Initialise the MinMaxScaler to scale features to the range [0, 1]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Fit the scaler on the feature data and transform it\n",
        "rescaledX = scaler.fit_transform(X)\n",
        "\n",
        "# Display the scaled feature matrix\n",
        "rescaledX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standardise data\n",
        "\n",
        "*Standardisation* is another way to adjust your data so it's easier for a machine learning model to work with. Instead of changing the range to 0–1 (like rescaling), standardisation shifts the data so that each feature has:\n",
        "- A *mean* (average) of 0  \n",
        "- A *standard deviation* of 1 (which measures how spread out the values are)\n",
        "\n",
        "This doesn’t change the shape of the distribution, but it re-centres the data around zero and gives it a consistent scale. This is especially useful when features are measured in very different units or ranges.\n",
        "\n",
        "Standardisation is helpful for:\n",
        "- *Making sure features with large numbers don’t dominate smaller ones*  \n",
        "- *Helping models that rely on distance or assume normally distributed data—like linear regression, logistic regression, and support vector machines*  \n",
        "- *Speeding up learning and improving accuracy in many cases*\n",
        "\n",
        "Imagine you’re analysing housing data. One feature might be *number of bedrooms* (typically between 1 and 5), and another might be *property value* (ranging into the hundreds of thousands). Standardising puts them on a level playing field so the model can treat both features fairly and interpret their influence correctly.\n",
        "\n",
        "When you bring everything into a consistent form, standardisation makes it easier for your model to find patterns and make more reliable predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(X)\n",
        "rescaledX = scaler.transform(X)\n",
        "\n",
        "print(rescaledX[0:5,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalise data\n",
        "\n",
        "Normalisation is a transformation that adjusts each row of your dataset so that all its values are scaled relative to one another. In simple terms, it makes sure that each data point (or row) has the same overall size or *length*, usually set to 1. This is also called giving it a *unit norm*.\n",
        "\n",
        "Unlike rescaling or standardising—which adjust each column—normalisation works across each row, balancing all the values in that row so they can be fairly compared.\n",
        "\n",
        "This is especially useful when:\n",
        "- *Your data is sparse* (meaning most of the values are zero or very small)  \n",
        "- *You're using models that rely on measuring distances between points*, like k-nearest neighbours or clustering algorithms  \n",
        "- *You want to compare patterns in proportions rather than absolute values*\n",
        "\n",
        "For example, imagine you're comparing music streaming habits across users. One person might listen to 50 songs a day, while another listens to just 5. But if you normalise the data, you’re not comparing total listening time—you’re comparing *how each person divides their listening across different genres or artists*. This allows you to focus on their preferences rather than how much they listen.\n",
        "\n",
        "Normalisation is helpful when the *direction* or *pattern* of the values matters more than their total size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "scaler = Normalizer().fit(X)\n",
        "normalisedX = scaler.transform(X)\n",
        "\n",
        "print(normalisedX[0:5,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binarise data\n",
        "\n",
        "Binarisation is a technique that turns numerical values into simple 0s and 1s based on a set threshold. If a value is *above* the threshold, it becomes 1. If it’s *below* or equal to the threshold, it becomes 0.\n",
        "\n",
        "This can be useful when you want to highlight the *presence or absence* of something, or turn a continuous variable into a yes/no type format.\n",
        "\n",
        "Binarisation is helpful for:\n",
        "- *Simplifying features into clear categories*  \n",
        "- *Preparing data for models that work better with binary input*  \n",
        "- *Highlighting specific conditions, like values above a healthy range*\n",
        "\n",
        "For example, imagine you're working with temperature data. You could set a threshold of 0°C to create a new feature that tells you whether it was *freezing* (1) or *not freezing* (0) on each day. This makes it easy for a model to focus on a specific condition without dealing with the full range of temperatures.\n",
        "\n",
        "It’s a straightforward way to turn continuous values into something simpler and more focused:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "binariser = Binarizer(threshold=0.0).fit(X)\n",
        "binaryX = binariser.transform(X)\n",
        "\n",
        "print(binaryX[0:5,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Influence of data transformation on ML models\n",
        "\n",
        "Data transformations can have a big impact on how well machine learning models perform—especially those that rely on patterns, distances, or consistent scales across features.\n",
        "\n",
        "In this example, we look at how *normalising* and *scaling* the data influences the performance of a basic decision tree model and a K-nearest neighbours model. We compare results *before* and *after* normalisation and standardisation to see if the transformations make any difference to the accuract of our model.\n",
        "\n",
        "While decision trees aren’t usually sensitive to feature scales, this test helps illustrate that:\n",
        "- *Some models (like decision trees)* may not change much in terms of accuracy, but this depends on the data.  \n",
        "- *Other models (like k-nearest neighbours, logistic regression, or neural networks)* can be heavily affected  \n",
        "- Even small differences in transformation can shift model accuracy, speed, or consistency\n",
        "\n",
        "Running this kind of test is a quick way to check whether a transformation improves model behaviour—and helps you choose the right preprocessing steps for your task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation (measuring accuracy)\n",
        "from sklearn.tree import DecisionTreeClassifier              # Decision tree model\n",
        "from sklearn.preprocessing import Normalizer                 # Normaliser for feature scaling\n",
        "\n",
        "# Set up 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10)\n",
        "\n",
        "# Initialise a Decision Tree Classifier\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Evaluate model using original (unscaled) data\n",
        "results1 = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(\"Mean estimated accuracy (original data):\", results1.mean())\n",
        "\n",
        "# Normalise the feature data (L2 norm by default)\n",
        "scaler = Normalizer().fit(X)\n",
        "normalisedX = scaler.transform(X)\n",
        "\n",
        "# Evaluate model using normalised data\n",
        "results2 = cross_val_score(model, normalisedX, Y, cv=kfold)\n",
        "print(\"Mean estimated accuracy (normalised data):\", results2.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since KNN is a distance-based model, it's often very sensitive to the scale of the input data—so you should see a noticeable difference in accuracy after applying a standard scaler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import KFold, cross_val_score      # For cross-validation (for accuracy measures)\n",
        "from sklearn.neighbors import KNeighborsClassifier              # K-Nearest Neighbors model\n",
        "from sklearn.preprocessing import StandardScaler                # StandardScaler for feature scaling\n",
        "\n",
        "# Set up 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10)\n",
        "\n",
        "# Initialise a KNN classifier\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "# Evaluate the model using original (unscaled) data\n",
        "results1 = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(\"Mean estimated accuracy (original data):\", results1.mean())\n",
        "\n",
        "# Standardise the feature data (zero mean, unit variance)\n",
        "scaler = StandardScaler().fit(X)\n",
        "standardisedX = scaler.transform(X)\n",
        "\n",
        "# Evaluate the model using standardised data\n",
        "results2 = cross_val_score(model, standardisedX, Y, cv=kfold)\n",
        "print(\"Mean estimated accuracy (standardised data):\", results2.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What have we learnt?\n",
        "\n",
        "Numeric data is structured and typically easier for machine learning models to handle than text. It forms the basis of most traditional machine learning tasks, such as predicting house prices, detecting fraud, or classifying medical outcomes. \n",
        "\n",
        "We started by exploring how to inspect and understand numeric datasets using tools like pandas. This included loading, and checking the shape of the dataset, and viewing column types with `.info()`. These initial steps help us identify missing values before we consider preprocessing our data.\n",
        "\n",
        "However, before we can use numeric data effectively, we must understand its structure, clean it, and apply the right transformations. We've seen how important it is to prepare and transform data before applying machine learning algorithms. Even small changes—like adjusting the scale or structure of the data—can make a noticeable difference in how models perform. This is because:\n",
        "\n",
        "- *Real-world data is rarely ready “as-is”* — it often needs cleaning and transforming to work well with machine learning models.\n",
        "- *Different transformations serve different purposes* — rescaling, standardising, normalising, and binarising each help in their own way, depending on the model and the data.\n",
        "- *Some models are sensitive to scale* — algorithms like K-nearest neighbours and logistic regression perform better when features are on a similar scale. Other models, like decision trees, are less affected — but it’s still good practice to test and compare.\n",
        "\n",
        "In short, data transformation isn't just a technical step—it’s a key part of the modelling process. The better you understand your data and how to prepare it, the more likely your model is to succeed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMJTQ3RqrsOY"
      },
      "source": [
        "## Recommended datasets\n",
        "\n",
        "Here are some widely used, freely available numeric datasets you might explore for practice:\n",
        "\n",
        "**Iris**  \n",
        "- *Description*: Classic dataset of 150 iris flowers with four features each (sepal length, sepal width, petal length, petal width) and three species of iris as the target.  \n",
        "- *Why it’s popular*: Very small (perfect for quick demos) and well-labelled, making it easy to visualise in 2D or 3D.  \n",
        "- *Where to get it*: Built into scikit-learn (`sklearn.datasets.load_iris`) or from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Iris).\n",
        "\n",
        "**Wine**  \n",
        "- *Description*: Chemical analysis of wines grown in the same region in Italy but from three different cultivars. Each sample has 13 numeric features.  \n",
        "- *Why it’s popular*: Good example for classification with multiple classes.  \n",
        "- *Where to get it*: Built into scikit-learn (`sklearn.datasets.load_wine`) or from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wine).\n",
        "\n",
        "**Wine Quality**  \n",
        "- *Description*: Wine samples (red or white) with attributes such as acidity, sulphates, pH, and a quality rating.  \n",
        "- *Why it’s popular*: Demonstrates regression or classification.  \n",
        "- *Where to get it*: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality).\n",
        "\n",
        "**Adult**  \n",
        "- *Description*: Census data (48,842 instances) used for predicting whether an individual’s income exceeds $50K/year.  \n",
        "- *Why it’s popular*: Classification with categorical and numeric features, plus data-cleaning challenges.  \n",
        "- *Where to get it*: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult).\n",
        "\n",
        "**Titanic**  \n",
        "- *Description*: Passenger data about who survived/perished on the RMS Titanic.  \n",
        "- *Why it’s popular*: Classic Kaggle competition for beginners, with mixed feature types and missing data.  \n",
        "- *Where to get it*: [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
